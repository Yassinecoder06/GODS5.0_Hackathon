{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa91a2f0",
   "metadata": {},
   "source": [
    "# Phase 2 (RoBERTa): Multi-Label Pipeline (Macro-F1)\n",
    "\n",
    "Standalone RoBERTa version of Phase 2.\n",
    "\n",
    "Outputs:\n",
    "- `outputs/submission_roberta.csv`\n",
    "- `outputs/roberta_threshold_report.json`\n",
    "- `outputs/oof_probs_roberta.csv`\n",
    "- `outputs/test_probs_roberta.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51958d21",
   "metadata": {},
   "source": [
    "## 1) Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed in Colab, uncomment:\n",
    "# !pip install -q transformers datasets accelerate iterative-stratification scikit-learn pandas numpy tqdm matplotlib\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print('Torch:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f9667",
   "metadata": {},
   "source": [
    "## 2) Configure Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b260030",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['E', 'S', 'G', 'non_ESG']\n",
    "ID_COL = 'id'\n",
    "TEXT_COL = 'text'\n",
    "\n",
    "MODEL_NAME = 'roberta-base'\n",
    "MAX_LENGTH = 256\n",
    "N_SPLITS = 5\n",
    "LR = 2e-5\n",
    "TRAIN_BS = 12\n",
    "EVAL_BS = 24\n",
    "NUM_EPOCHS = 3\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "THRESHOLD_MIN = 0.05\n",
    "THRESHOLD_MAX = 0.95\n",
    "THRESHOLD_STEP = 0.01\n",
    "ENABLE_NON_ESG_RULE = True\n",
    "\n",
    "if Path('data_set').exists():\n",
    "    ROOT = Path('.')\n",
    "elif Path('../data_set').exists():\n",
    "    ROOT = Path('..')\n",
    "else:\n",
    "    ROOT = Path('.')\n",
    "\n",
    "TRAIN_PATH = ROOT / 'data_set' / 'train.csv'\n",
    "TEST_PATH = ROOT / 'data_set' / 'test.csv'\n",
    "SAMPLE_SUB_PATH = ROOT / 'data_set' / 'sample_submission.csv'\n",
    "OUTPUT_DIR = ROOT / 'outputs'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('ROOT:', ROOT.resolve())\n",
    "print('TRAIN_PATH exists:', TRAIN_PATH.exists())\n",
    "print('TEST_PATH exists:', TEST_PATH.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f9bd69",
   "metadata": {},
   "source": [
    "## 3) Load or Create Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "sample_sub_df = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "print('Train shape:', train_df.shape)\n",
    "print('Test shape:', test_df.shape)\n",
    "print('Sample submission shape:', sample_sub_df.shape)\n",
    "\n",
    "required_train_cols = [ID_COL, TEXT_COL] + LABELS\n",
    "missing_cols = [c for c in required_train_cols if c not in train_df.columns]\n",
    "assert len(missing_cols) == 0, f'Missing train columns: {missing_cols}'\n",
    "assert ID_COL in test_df.columns and TEXT_COL in test_df.columns\n",
    "\n",
    "train_df[TEXT_COL] = train_df[TEXT_COL].fillna('').astype(str)\n",
    "test_df[TEXT_COL] = test_df[TEXT_COL].fillna('').astype(str)\n",
    "\n",
    "y = train_df[LABELS].values.astype(np.float32)\n",
    "prevalence = train_df[LABELS].mean().sort_values(ascending=False)\n",
    "print('\\nLabel prevalence:')\n",
    "print(prevalence)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b9d81",
   "metadata": {},
   "source": [
    "## 4) Implement Core Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f187c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class ESGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length, labels=None):\n",
    "        self.texts = list(texts)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    scores = [f1_score(y_true[:, i], y_pred[:, i], zero_division=0) for i in range(y_true.shape[1])]\n",
    "    return float(np.mean(scores)), scores\n",
    "\n",
    "\n",
    "def tune_thresholds(y_true, y_prob, tmin=0.05, tmax=0.95, step=0.01):\n",
    "    grid = np.arange(tmin, tmax + 1e-12, step)\n",
    "    best = {}\n",
    "    for i, label in enumerate(LABELS):\n",
    "        best_t, best_f1 = 0.5, -1.0\n",
    "        for t in grid:\n",
    "            pred = (y_prob[:, i] >= t).astype(int)\n",
    "            f1 = f1_score(y_true[:, i], pred, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_t = float(t)\n",
    "        best[label] = round(best_t, 4)\n",
    "    return best\n",
    "\n",
    "\n",
    "def apply_thresholds(y_prob, thresholds):\n",
    "    y_pred = np.zeros_like(y_prob, dtype=int)\n",
    "    for i, label in enumerate(LABELS):\n",
    "        y_pred[:, i] = (y_prob[:, i] >= thresholds[label]).astype(int)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def apply_non_esg_rule(y_pred):\n",
    "    fixed = y_pred.copy()\n",
    "    esg_any = (fixed[:, 0] + fixed[:, 1] + fixed[:, 2]) > 0\n",
    "    fixed[esg_any, 3] = 0\n",
    "    return fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b82992",
   "metadata": {},
   "outputs": [],
   "source": [
    "mskf = MultilabelStratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_prob = np.zeros((len(train_df), len(LABELS)), dtype=np.float32)\n",
    "test_prob_folds = []\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(mskf.split(train_df[TEXT_COL].values, y), start=1):\n",
    "    print(f'\\n===== Fold {fold}/{N_SPLITS} =====')\n",
    "\n",
    "    x_tr = train_df.iloc[tr_idx][TEXT_COL].tolist()\n",
    "    x_va = train_df.iloc[va_idx][TEXT_COL].tolist()\n",
    "    y_tr = y[tr_idx]\n",
    "    y_va = y[va_idx]\n",
    "\n",
    "    train_ds = ESGDataset(x_tr, tokenizer, MAX_LENGTH, y_tr)\n",
    "    valid_ds = ESGDataset(x_va, tokenizer, MAX_LENGTH, y_va)\n",
    "    test_ds = ESGDataset(test_df[TEXT_COL].tolist(), tokenizer, MAX_LENGTH, labels=None)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(LABELS),\n",
    "        problem_type='multi_label_classification'\n",
    "    )\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(OUTPUT_DIR / f'roberta_fold_{fold}'),\n",
    "        learning_rate=LR,\n",
    "        per_device_train_batch_size=TRAIN_BS,\n",
    "        per_device_eval_batch_size=EVAL_BS,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='no',\n",
    "        logging_steps=50,\n",
    "        report_to='none',\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=2,\n",
    "        remove_unused_columns=False,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=valid_ds,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    va_logits = trainer.predict(valid_ds).predictions\n",
    "    te_logits = trainer.predict(test_ds).predictions\n",
    "\n",
    "    va_prob = sigmoid(va_logits)\n",
    "    te_prob = sigmoid(te_logits)\n",
    "\n",
    "    oof_prob[va_idx] = va_prob\n",
    "    test_prob_folds.append(te_prob)\n",
    "\n",
    "    fold_pred_default = (va_prob >= 0.5).astype(int)\n",
    "    fold_macro, fold_label_scores = macro_f1(y_va.astype(int), fold_pred_default)\n",
    "    fold_metrics.append({\n",
    "        'fold': fold,\n",
    "        'macro_f1@0.5': round(fold_macro, 6),\n",
    "        'label_f1@0.5': {LABELS[i]: round(float(fold_label_scores[i]), 6) for i in range(len(LABELS))}\n",
    "    })\n",
    "    print('Fold macro-F1 @0.5:', round(fold_macro, 6))\n",
    "\n",
    "    del model, trainer, train_ds, valid_ds, test_ds\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "test_prob = np.mean(np.stack(test_prob_folds, axis=0), axis=0)\n",
    "print('\\nFinished CV. OOF shape:', oof_prob.shape, ' Test prob shape:', test_prob.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522df6a",
   "metadata": {},
   "source": [
    "## 5) Run Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f14b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_int = y.astype(int)\n",
    "thresholds = tune_thresholds(y_true_int, oof_prob, THRESHOLD_MIN, THRESHOLD_MAX, THRESHOLD_STEP)\n",
    "\n",
    "oof_pred = apply_thresholds(oof_prob, thresholds)\n",
    "test_pred = apply_thresholds(test_prob, thresholds)\n",
    "\n",
    "macro_base, per_label_base = macro_f1(y_true_int, oof_pred)\n",
    "print('Macro-F1 after threshold tuning:', round(macro_base, 6))\n",
    "\n",
    "rule_applied = False\n",
    "macro_after_rule = macro_base\n",
    "if ENABLE_NON_ESG_RULE:\n",
    "    oof_pred_rule = apply_non_esg_rule(oof_pred)\n",
    "    macro_rule, _ = macro_f1(y_true_int, oof_pred_rule)\n",
    "    print('Macro-F1 after non_ESG rule:', round(macro_rule, 6))\n",
    "    if macro_rule >= macro_base:\n",
    "        oof_pred = oof_pred_rule\n",
    "        test_pred = apply_non_esg_rule(test_pred)\n",
    "        macro_after_rule = macro_rule\n",
    "        rule_applied = True\n",
    "\n",
    "assert oof_prob.shape == (len(train_df), len(LABELS))\n",
    "assert test_prob.shape == (len(test_df), len(LABELS))\n",
    "assert set(thresholds.keys()) == set(LABELS)\n",
    "print('Validation checks passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3649f4b",
   "metadata": {},
   "source": [
    "## 6) Visualize Key Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2835b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame({\n",
    "    'label': LABELS,\n",
    "    'prevalence': [float(prevalence[l]) for l in LABELS],\n",
    "    'threshold': [float(thresholds[l]) for l in LABELS],\n",
    "    'f1_oof': [float(per_label_base[i]) for i in range(len(LABELS))],\n",
    "}).sort_values('prevalence', ascending=False)\n",
    "\n",
    "display(summary_df)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "summary_df.plot(kind='bar', x='label', y='prevalence', ax=axes[0], legend=False, title='Label Prevalence')\n",
    "summary_df.plot(kind='bar', x='label', y='threshold', ax=axes[1], legend=False, title='Tuned Thresholds')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a40fd",
   "metadata": {},
   "source": [
    "## 7) Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191df18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = sample_sub_df.copy()\n",
    "if ID_COL in submission.columns and ID_COL in test_df.columns:\n",
    "    submission[ID_COL] = test_df[ID_COL].values\n",
    "for i, label in enumerate(LABELS):\n",
    "    submission[label] = test_pred[:, i].astype(int)\n",
    "\n",
    "oof_prob_df = pd.DataFrame({ID_COL: train_df[ID_COL].values})\n",
    "for i, label in enumerate(LABELS):\n",
    "    oof_prob_df[f'{label}_prob'] = oof_prob[:, i]\n",
    "\n",
    "test_prob_df = pd.DataFrame({ID_COL: test_df[ID_COL].values})\n",
    "for i, label in enumerate(LABELS):\n",
    "    test_prob_df[f'{label}_prob'] = test_prob[:, i]\n",
    "\n",
    "report = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'labels': LABELS,\n",
    "    'thresholds': {k: float(v) for k, v in thresholds.items()},\n",
    "    'macro_f1_oof_threshold_tuned': float(macro_base),\n",
    "    'macro_f1_oof_after_optional_non_esg_rule': float(macro_after_rule),\n",
    "    'non_esg_rule_applied': bool(rule_applied),\n",
    "    'fold_metrics_default_threshold_0_5': fold_metrics,\n",
    "}\n",
    "\n",
    "submission_path = OUTPUT_DIR / 'submission_roberta.csv'\n",
    "report_path = OUTPUT_DIR / 'roberta_threshold_report.json'\n",
    "oof_path = OUTPUT_DIR / 'oof_probs_roberta.csv'\n",
    "test_prob_path = OUTPUT_DIR / 'test_probs_roberta.csv'\n",
    "\n",
    "submission.to_csv(submission_path, index=False)\n",
    "oof_prob_df.to_csv(oof_path, index=False)\n",
    "test_prob_df.to_csv(test_prob_path, index=False)\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print('Saved:')\n",
    "print(submission_path)\n",
    "print(report_path)\n",
    "print(oof_path)\n",
    "print(test_prob_path)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
